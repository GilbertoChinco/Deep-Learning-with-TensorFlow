{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLTF_05_Convolutional_Neural_Networks_Part_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning: Convolutional Neural Networks applied on MNIST\n",
        "\n",
        "6. Deep learning applied on MNIST\n",
        "7. Summary of the deep learning Neural Network\n",
        "8. Define functions and train the model\n",
        "9. Evaluate the model"
      ],
      "metadata": {
        "id": "y443kYB2r70H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Deep learning applied on MNIST\n",
        "\n",
        "In the first part, we learned how to use a simple ANN to classify MNIST. Now we are going to expand our knowledge using a Deep Neural Network.\n",
        "\n",
        "Architecture of our network is:\n",
        "\n",
        "*   (Input) -> \\[batch_size, 28, 28, 1]  >> Apply 32 filter of \\[5x5]\n",
        "*   (Convolutional layer 1)  -> \\[batch_size, 28, 28, 32]\n",
        "*   (ReLU 1)  -> \\[?, 28, 28, 32]\n",
        "*   (Max pooling 1) -> \\[?, 14, 14, 32]\n",
        "*   (Convolutional layer 2)  -> \\[?, 14, 14, 64]\n",
        "*   (ReLU 2)  -> \\[?, 14, 14, 64]\n",
        "*   (Max pooling 2)  -> \\[?, 7, 7, 64]\n",
        "*   \\[fully connected layer 3] -> \\[1x1024]\n",
        "*   \\[ReLU 3]  -> \\[1x1024]\n",
        "*   \\[Drop out]  -> \\[1x1024]\n",
        "*   \\[fully connected layer 4] -> \\[1x10]\n",
        "\n",
        "The next cells will explore this new architecture."
      ],
      "metadata": {
        "id": "Lwsmf9JWsIYj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wOqBhFyrwoH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "x-35YRbXtAyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "R0Lv4CG6tHP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.one_hot(y_train, 10)\n",
        "y_test = tf.one_hot(y_test, 10)"
      ],
      "metadata": {
        "id": "stIbNGh0tP6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial parameters**\n",
        "\n",
        "Create general parameters for the model"
      ],
      "metadata": {
        "id": "dCrAj3y2tZRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimension for the image shape\n",
        "width = 28\n",
        "height = 28\n",
        "flat = width * height   #Number of pixel in one image\n",
        "class_output = 10       #Number of labels"
      ],
      "metadata": {
        "id": "D_GPRlSFtSwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting images of the data set to tensor**\n",
        "\n",
        "The input image is 28 pixels by 28 pixels, 1 channel (grayscale). In this case, the first dimension is the batch number of the image, and can be of any size (so we set it to -1). The second and third dimensions are width and height, and the last one is the image channels."
      ],
      "metadata": {
        "id": "WbUC60kJt7Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_image_train = tf.reshape(x_train, [-1, 28, 28, 1])\n",
        "x_image_train = tf.cast(x_image_train, \"float32\")\n",
        "\n",
        "x_image_test = tf.reshape(x_test, [-1, 28, 28, 1])\n",
        "x_image_test = tf.cast(x_image_test, \"float32\")\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_image_train, y_train)).batch(50)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_image_test, y_test)).batch(50)"
      ],
      "metadata": {
        "id": "NpQ6MEEetusT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_image_train = tf.slice(x_image_train, [0, 0, 0, 0], [10000, 28, 28, 1])\n",
        "y_train = tf.slice(y_train, [0, 0], [10000, 10])"
      ],
      "metadata": {
        "id": "QK13Qamju9Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layer 1\n",
        "\n",
        "**Defining kernel weights and bias**\n",
        "\n",
        "We define a kernel here. The Size of the filter/kernel is 5x5; Input channels is 1 (grayscale); and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. So, the output of convolution layer would be 28x28x32). In this step, we create a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`"
      ],
      "metadata": {
        "id": "zkJAQL1GvaYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev = 0.1, seed = 0))\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape = [32]))"
      ],
      "metadata": {
        "id": "cFOxCyGsvRpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://ibm.box.com/shared/static/vn26neef1nnv2oxn5cb3uueowcawhkgb.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n",
        "\n",
        "To create convolutional layer, we use `tf.nn.conv2d`. It computes a 2-D convolution given 4-D input and filter tensors.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "*   tensor of shape \\[batch, in_height, in_width, in_channels]. x of shape \\[batch_size,28 ,28, 1]\n",
        "\n",
        "*   a filter / kernel tensor of shape \\[filter_height, filter_width, in_channels, out_channels]. W is of size \\[5, 5, 1, 32]\n",
        "\n",
        "*   stride which is  \\[1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. As the input tensor has 4 dimensions:  \\[batch, height, width, channels], then the convolution operates on a 2D window on the height and width dimensions. **strides** determines how much the window shifts by in each of the dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we could set other values, e.g. \\[1, 2, 2, 1]\n",
        "\n",
        "Process:\n",
        "\n",
        "*   Change the filter to a 2-D matrix with shape \\[5\\*5\\*1,32]\n",
        "*   Extracts image patches from the input tensor to form a *virtual* tensor of shape `[batch, 28, 28, 5*5*1]`.\n",
        "*   For each batch, right-multiplies the filter matrix and the image vector.\n",
        "\n",
        "Output:\n",
        "\n",
        "*   A `Tensor` (a 2-D convolution) of size tf.Tensor 'add\\_7:0' shape=(?, 28, 28, 32)- Notice: the output of the first convolution layer is 32 \\[28x28] images. Here 32 is considered as volume/depth of the output image."
      ],
      "metadata": {
        "id": "y_1WA-cowipT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve1(x):\n",
        "  return tf.nn.conv2d(x, W_conv1, strides = [1, 1, 1, 1], padding = \"SAME\") + b_conv1"
      ],
      "metadata": {
        "id": "MVJ5HWjywHJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://ibm.box.com/shared/static/iizf4ui4b2hh9wn86pplqxu27ykpqci9.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >"
      ],
      "metadata": {
        "id": "4K-TiT5MygQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply the ReLU activation function**\n",
        "\n",
        "In this step, we just go through all outputs convolution layer, convolve1, and wherever a negative number occurs, we swap it out for a 0. It is called ReLU activation Function.\n",
        "Let f(x) is a ReLU activation function:\n",
        "\n",
        "$f(x) = max(0, X)$"
      ],
      "metadata": {
        "id": "5a3GOLOZxOA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_function_relu(x):\n",
        "  return tf.nn.relu(convolve1(x))"
      ],
      "metadata": {
        "id": "fut36nnuxC-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aplly the max pooling**\n",
        "\n",
        "max pooling is a form of non-linear down-sampling. It partitions the input image into a set of rectangles and, then find the maximum value for that region.\n",
        "\n",
        "Lets use `tf.nn.max_pool` function to perform max pooling. Kernel size: 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)\\ Strides: dictates the sliding behaviour of the kernel. In this case it will move 2 pixels everytime, thus not overlapping. The input is a matrix of size 28x28x32, and the output would be a matrix of size 14x14x32.\n",
        "\n",
        "<img src=\"https://ibm.box.com/shared/static/kmaja90mn3aud9mro9cn8pbbg1h5pejy.png\" alt=\"HTML5 Icon\" style=\"width: 800px; height: 400px;\"> "
      ],
      "metadata": {
        "id": "vPGSSXdjyBGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv1(x):\n",
        "  return tf.nn.max_pool(act_function_relu(x), ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")"
      ],
      "metadata": {
        "id": "D7wJXrFQx8HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layer 2\n",
        "\n",
        "**Weights and Biases of Kernels**\n",
        "\n",
        "We apply the convolution again in this layer. Lets look at the second layer kernel:\n",
        "\n",
        "* Filter/kernel: 5x5 (25 pixels)\n",
        "* Input channels: 32 (from the 1st Conv layer, we had 32 feature maps)\n",
        "* 64 output feature maps\n",
        "\n",
        "\n",
        "Notice: here, the input image is [14x14x32], the filter is [5x5x32], we use 64 filters of size [5x5x32], and the output of the convolutional layer would be 64 convolved image, [14x14x64].\n",
        "\n",
        "Notice: the convolution result of applying a filter of size [5x5x32] on image of size [14x14x32] is an image of size [14x14x1], that is, the convolution is functioning on volume."
      ],
      "metadata": {
        "id": "h5VJNvP0zKqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev = 0.1, seed = 0))\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape = [64]))"
      ],
      "metadata": {
        "id": "gtY7vbSrzCOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolve image weight tensor and add biases"
      ],
      "metadata": {
        "id": "__lqjARn0KAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve2(x):\n",
        "  return tf.nn.conv2d(conv1(x), W_conv2, strides = [1, 1, 1, 1], padding = \"SAME\") + b_conv2"
      ],
      "metadata": {
        "id": "r5cA_1GIz-LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "applt the ReLU activation function"
      ],
      "metadata": {
        "id": "sIYnb5QX0RP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_function_relu2(x):\n",
        "  return (tf.nn.relu(convolve2(x)))"
      ],
      "metadata": {
        "id": "XmbwY1Df0RrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the max pooling"
      ],
      "metadata": {
        "id": "5qoykCNd0agF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2(x):\n",
        "  return tf.nn.max_pool(act_function_relu2(x), ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")"
      ],
      "metadata": {
        "id": "1gqxazOI0ZMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the second layer it is a 64 matrix of [7x7]"
      ],
      "metadata": {
        "id": "LdElSs4B0noZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full connected layer\n",
        "\n",
        "You need a fully connected layer to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all 64 matrices, and convert them to a flat array.\n",
        "\n",
        "So, each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, which make an array of size [3136x1]. We will connect it into another layer of size [1024x1]. So, the weight between these 2 layers will be [3136x1024]\n",
        "\n",
        "<img src=\"https://ibm.box.com/shared/static/pr9mnirmlrzm2bitf1d4jj389hyvv7ey.png\" alt=\"HTML5 Icon\" style=\"width: 800px; height: 400px;\"> "
      ],
      "metadata": {
        "id": "Sh_quY1Z04Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flattering Second layer**\n"
      ],
      "metadata": {
        "id": "kH-umVFd1pRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer2_matrix(x):\n",
        "  return tf.reshape(conv2(x), [-1, 7 * 7 * 64])"
      ],
      "metadata": {
        "id": "ZwJmHeTC1Tlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weigths and Biases between lñayer 2 and 3**\n",
        "\n",
        "Composition of the feature map from the last layer (7x7) multiplied by the number of feature maps (64); 1027 outputs to Softmax layer"
      ],
      "metadata": {
        "id": "lMRdwEfp2CLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev = 0.1, seed = 2))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1, shape = [1024]))"
      ],
      "metadata": {
        "id": "ajHgIYeT18Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix multipplication (applying weightd and biases)**\n"
      ],
      "metadata": {
        "id": "U-PK6ofh2dCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc1(x):\n",
        "  return tf.matmul(layer2_matrix(x), w_fc1) + b_fc1"
      ],
      "metadata": {
        "id": "gL_71TGB2a-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply the ReLU activation function**\n"
      ],
      "metadata": {
        "id": "ZK1AUcPK2u9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_function_relu3(x):\n",
        "  return tf.nn.relu(fc1(x))"
      ],
      "metadata": {
        "id": "Fg-9RVSN2scN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout Layer\n",
        "\n",
        "This is an optional phase for reducing overfitting\n",
        "\n",
        "It is a phase where the network \"forget\" some features. At each training step in a mini-batch, some units get switched off randomly so that it will not interact with the network. That is, it weights cannot be updated, nor affect the learning of the other network nodes. This can be very useful for very large neural networks to prevent overfitting."
      ],
      "metadata": {
        "id": "B3nrtEIw2_PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_prob = 0.5\n",
        "\n",
        "def layer_drop(x):\n",
        "  return tf.nn.dropout(act_function_relu3(x), keep_prob)"
      ],
      "metadata": {
        "id": "IgPImqDQ28-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Readout Layer (Softmaw Layer)**\n",
        "\n",
        "Type: Softmax, fully connected layer\n",
        "\n",
        "**Weight and biases**\n",
        "\n",
        "In last layer, CNN takes the high-level filtered images and translate them into votes using softmax. Input channels: 1024 (neurons from the 3rd Layer); 10 output features"
      ],
      "metadata": {
        "id": "jRsRHqCy3diM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev = 0.1, seed = 2))\n",
        "b_fc2 = tf.Variable(tf.constant(0.1, shape = [10]))"
      ],
      "metadata": {
        "id": "eTW-X9bN3Y5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix multipplication (applying weightd and biases)**\n"
      ],
      "metadata": {
        "id": "QMSsqUI84G6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc2(x):\n",
        "  return tf.matmul(layer_drop(x), w_fc2) + b_fc2"
      ],
      "metadata": {
        "id": "ppmHCJUh4CXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply the softmax activation function**"
      ],
      "metadata": {
        "id": "tNp-YX7S4RQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def y_softmax(x):\n",
        "  return tf.nn.softmax(fc2(x))"
      ],
      "metadata": {
        "id": "vBDq3f5f4QYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Summary of the Deep Convolutional Neural Network\n",
        "\n",
        "* Input - MNIST dataset\n",
        "* Convolutional and Max-pooling\n",
        "* Convolutional and Max-pooling\n",
        "* Fully Connected layer\n",
        "* Processing - Dropout\n",
        "* Readout layer - Fully connected\n",
        "* Outputs - Classified digits"
      ],
      "metadata": {
        "id": "kncI3pNa496W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Define functions and train the model"
      ],
      "metadata": {
        "id": "LCTYRXEQ5ufa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the loss function**\n",
        "\n",
        "We need to compare our output, layer4 tensor, with ground truth for all mini_batch. we can use cross entropy `b` to see how bad our CNN is working - to measure the error at a softmax layer.\n",
        "\n",
        "The following code shows an toy sample of cross-entropy for a mini-batch of size 2 which its items have been classified. You can run it (first change the cell type to code in the toolbar) to see how cross entropy changes."
      ],
      "metadata": {
        "id": "artirUzn6Kyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " \n",
        "layer4_test = [[0.9, 0.1, 0.1], [0.9, 0.1, 0.1]]\n",
        "y_test = [[1, 0, 0], [1, 0, 0]]\n",
        "\n",
        "np.mean(-np.sum(y_test * np.log(layer4_test), 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7zKMCU85VJh",
        "outputId": "2efce591-2c9e-49f3-e49d-83a326256b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10536051565782628"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduce_sum computes the sum of elements of y * tf.log(layer4) across second dimension of the tensor, and reduce_mean computes the mean of all elements in the tensor.\n",
        "\n",
        "$CrossEntropy = \\sum y_{label} \\cdot log(y_{prediction})$"
      ],
      "metadata": {
        "id": "kASVG6Ze68CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_label, y_pred):\n",
        "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))"
      ],
      "metadata": {
        "id": "KhVlMATs62PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the optimizer**\n",
        "\n",
        "It is obvious that we want minimize the error of our network which is calculated by cross_entropy metric. To solve the problem, we have to compute gradients for the loss (which is minimizing the cross-entropy) and apply gradients to variables. It will be done by an optimizer: **GradientDescent** or **Adagrad.**"
      ],
      "metadata": {
        "id": "uUhOokEW7Xon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "I675ucer7WF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the convention of our first example, we will use **GradientTape** to define a model."
      ],
      "metadata": {
        "id": "CBbCxnMH7lMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables = [W_conv1, b_conv1, W_conv2, b_conv2, \n",
        "             w_fc1, b_fc1, w_fc2, b_fc2, ]\n",
        "\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    current_loss = cross_entropy(y, y_softmax(x))\n",
        "    grads = tape.gradient(current_loss, variables)\n",
        "    optimizer.apply_gradients(zip(grads, variables))\n",
        "    return current_loss.numpy()"
      ],
      "metadata": {
        "id": "P4Fni38h7jTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define prediction**"
      ],
      "metadata": {
        "id": "n9IS91wM8ntY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_prediction = tf.equal(tf.argmax(y_softmax(x_image_train), axis=1), tf.argmax(y_train, axis=1))"
      ],
      "metadata": {
        "id": "M4if5ijy8WFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define accuracy**\n",
        "\n"
      ],
      "metadata": {
        "id": "Uh3ZHIUu-5OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))"
      ],
      "metadata": {
        "id": "9hcUMeB9-mxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run session and train**\n"
      ],
      "metadata": {
        "id": "BklCVio3_ANH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values=[]\n",
        "accuracies = []\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    batch = 0\n",
        "    # each batch has 50 examples\n",
        "    for x_train_batch, y_train_batch in train_ds:\n",
        "        batch += 1\n",
        "        current_loss = train_step(x_train_batch, y_train_batch)\n",
        "\n",
        "        if batch % 50 == 0: #reporting intermittent batch statistics\n",
        "            correct_prediction = tf.equal(tf.argmax(y_softmax(x_train_batch), axis=1),\n",
        "                                  tf.argmax(y_train_batch, axis=1))\n",
        "            #  accuracy\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "            print(\"epoch \", str(epoch), \"batch\", str(batch), \"loss:\", str(current_loss),\n",
        "                     \"accuracy\", str(accuracy)) \n",
        "            \n",
        "    current_loss = cross_entropy( y_train, y_softmax( x_image_train )).numpy()\n",
        "    loss_values.append(current_loss)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_softmax(x_image_train), axis=1),\n",
        "                                  tf.argmax(y_train, axis=1))\n",
        "    #  accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "    accuracies.append(accuracy)\n",
        "    print(\"end of epoch \", str(epoch), \"loss\", str(current_loss), \"accuracy\", str(accuracy) ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "s4msH0bq_CJ4",
        "outputId": "5af17708-09a0-4a00-9947-5ca06d1ced1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0 batch 50 loss: 141.75606 accuracy 0.32\n",
            "epoch  0 batch 100 loss: 59.031296 accuracy 0.72\n",
            "epoch  0 batch 150 loss: 35.671795 accuracy 0.74\n",
            "epoch  0 batch 200 loss: 35.593155 accuracy 0.82\n",
            "epoch  0 batch 250 loss: 44.849487 accuracy 0.78\n",
            "epoch  0 batch 300 loss: 28.509315 accuracy 0.82\n",
            "epoch  0 batch 350 loss: 31.482727 accuracy 0.84\n",
            "epoch  0 batch 400 loss: 12.52626 accuracy 0.9\n",
            "epoch  0 batch 450 loss: 19.943563 accuracy 0.84\n",
            "epoch  0 batch 500 loss: 11.631872 accuracy 0.92\n",
            "epoch  0 batch 550 loss: 10.699755 accuracy 0.9\n",
            "epoch  0 batch 600 loss: 26.553905 accuracy 0.82\n",
            "epoch  0 batch 650 loss: 20.633434 accuracy 0.76\n",
            "epoch  0 batch 700 loss: 2.7538722 accuracy 0.94\n",
            "epoch  0 batch 750 loss: 23.169407 accuracy 0.86\n",
            "epoch  0 batch 800 loss: 13.845081 accuracy 0.92\n",
            "epoch  0 batch 850 loss: 20.085352 accuracy 0.9\n",
            "epoch  0 batch 900 loss: 13.578524 accuracy 0.92\n",
            "epoch  0 batch 950 loss: 12.643195 accuracy 0.92\n",
            "epoch  0 batch 1000 loss: 17.674732 accuracy 0.94\n",
            "epoch  0 batch 1050 loss: 5.237559 accuracy 0.94\n",
            "epoch  0 batch 1100 loss: 16.77515 accuracy 0.88\n",
            "epoch  0 batch 1150 loss: 9.247859 accuracy 0.94\n",
            "epoch  0 batch 1200 loss: 6.359898 accuracy 0.98\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-0dbdca3c89eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"end of epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evualuate the model**\n",
        "\n"
      ],
      "metadata": {
        "id": "vVABQzgRCJVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 0\n",
        "acccuracies=[]\n",
        "# evaluate accuracy by batch and average...reporting every 100th batch\n",
        "for x_train_batch, y_train_batch in train_ds:\n",
        "        batch += 1\n",
        "        correct_prediction = tf.equal(tf.argmax(y_softmax(x_train_batch), axis=1),\n",
        "                                  tf.argmax(y_train_batch, axis=1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "        accuracies.append(accuracy)\n",
        "        if batch % 100 == 0:\n",
        "            print(\"batch\", str(batch), \"accuracy\", str(accuracy)) \n",
        "\n",
        "print(\"accuracy of entire set\", str(np.mean(accuracies))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0hmj0TbCNFG",
        "outputId": "880af736-90bf-48e9-f142-e0ec704b42e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 100 accuracy 0.96\n",
            "batch 200 accuracy 1.0\n",
            "batch 300 accuracy 0.94\n",
            "batch 400 accuracy 0.98\n",
            "batch 500 accuracy 0.9\n",
            "batch 600 accuracy 0.96\n",
            "batch 700 accuracy 0.96\n",
            "batch 800 accuracy 0.9\n",
            "batch 900 accuracy 0.92\n",
            "batch 1000 accuracy 0.9\n",
            "batch 1100 accuracy 0.96\n",
            "batch 1200 accuracy 0.98\n",
            "accuracy of entire set 0.9401684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**"
      ],
      "metadata": {
        "id": "LFO-NpxuCSp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "do you want to look all the filters?"
      ],
      "metadata": {
        "id": "5dkmezv0HKXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernels = tf.reshape(tf.transpose(W_conv1, perm=[2, 3, 0,1]),[32, -1])"
      ],
      "metadata": {
        "id": "alCkUlfGCVdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --output-document utils1.py https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week2/data/utils.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8XHwgusFS1d",
        "outputId": "a6165ef0-3f6e-42bb-cecf-9a0664bf8088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-17 06:01:46--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week2/data/utils.py\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5097 (5.0K) [text/x-python]\n",
            "Saving to: ‘utils1.py’\n",
            "\n",
            "utils1.py           100%[===================>]   4.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-17 06:01:46 (489 MB/s) - ‘utils1.py’ saved [5097/5097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utils1\n",
        "import imp\n",
        "imp.reload(utils1)\n",
        "from utils1 import tile_raster_images\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "XYvhJ0jpCXaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.fromarray(tile_raster_images(kernels.numpy(), img_shape=(5, 5) ,tile_shape=(4, 8), tile_spacing=(1, 1)))\n",
        "### Plot image\n",
        "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
        "imgplot = plt.imshow(image)\n",
        "imgplot.set_cmap('gray')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "o7AQ7xa-FZzq",
        "outputId": "d3e55f4a-6e18-4b6e-9963-3aa3186ace6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x1296 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABA4AAAIKCAYAAACjnw1sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debRddX3//9dHIgSEgAwCbUCKRSVaBQUlDN8CKqOCorVQCihoQKHixCQIUmSJSqsyaIuVQYsDKgioyI/SapqqSLBWhtjIJFOQSRapzLJ/f3CzfvxcfMqQ906C38djLVbuPefmeTaQfc/JKzv3tmEYAgAAAPB4nrW4DwAAAABYchkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABA16RF+WCTJ08envOc55Q2p0yZUtpb4Prrry9vrrDCCuXNRx55pLyZJL/97W/Lm2uvvXZ5M0lWXHHF8mZrrbx52223lTeT5P777y9v3n333eXNJFlrrbXKm6usskp5c6mllipvJsldd901SrfaddddN0p36tSp5c3VVlutvJkkv/71r8ubDz/8cHnzvvvuK28myfz588ubyy+/fHnzj/7oj8qbyTjP1/PmzStvJsmyyy5b3rzmmmvKm6uvvnp5MxnnHBjrO4499NBD5c3nPe955c0bbrihvJkkr3zlK8ubV1xxRXnzuc99bnkzSZZbbrny5hivV5NxPgesu+665c0xnleScV4DjPXaeo011ijt3XnnnZk/f/7j/sJqi/LbMa6yyirDjjvuWNp83eteV9pbYM899yxvbrXVVuXNe++9t7yZJJdcckl587Of/Wx5M0ne8IY3lDef9az6i3FOOumk8maSzJkzp7z5rW99q7yZJJ/+9KfLm2Ocq2O9aPjqV79a3hxjPNx9993Lm0ly/PHHlzf33Xff8maSnHDCCeXNMV6IXHnlleXNJLn44ovLm5tvvnl58yMf+Uh5M0m23nrr8uaxxx5b3kySl7zkJeXNXXbZpbx58MEHlzeT5PLLLy9vjvEb/GSc8eiAAw4ob77rXe8qbybjPF+9+MUvLm+++c1vLm8mySte8Yry5lh/0DHG54AxXgNtueWW5c1knNcrY722rv7ceuyxx+b6669/3OHAX1UAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdC3UcNBa26619t+ttatba4dWHRQAAACwZHjaw0FrbakkJyfZPsm0JLu11qZVHRgAAACw+C3MFQevSnL1MAzXDsPwYJKvJtm55rAAAACAJcHCDAd/nOTGx7x/08Rt/z+ttRmttdmttdkPPPDAQjwcAAAAsKiN/sURh2E4ZRiGjYZh2GiZZZYZ++EAAACAQgszHNycZK3HvD914jYAAADgD8TCDAeXJlmvtfYnrbWlk+ya5LyawwIAAACWBJOe7k8chuHh1toBSS5MslSSU4dhuLLsyAAAAIDF7mkPB0kyDMN3k3y36FgAAACAJczoXxwRAAAAeOYyHAAAAABdhgMAAACgy3AAAAAAdC3UF0d8qlprmTSp9iH32GOP0t4C7373u8ubN954Y3nzoIMOKm8mySWXXFLenDx5cnkzSdZaa63y5p577lnePOSQQ8qbSbLmmmuWN7/1rW+VN5PkvvvuK2+eeeaZ5c2NNtqovJkkRx99dHlz1VVXLW+OZfXVVy9vnnrqqeXNJNlwww3Lmw8++GB5c+WVVy5vJsnFF19c3nzTm95U3rzjjjvKm0my++67lzcPOOCA8maSvOpVrxqlW+3mm28epTvGa4Btt922vJkk2223XXnzuOOOK2+OZYxjHeO15bRp08qbSfLyl7+8vDnG6/WxHHjggeXN17/+9eXNJFl33XXLm1dffXV5M0m++MUvlvbuvPPO7n2uOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACga9KifLBHHnkk8+fPL22ed955pb0FDjrooPLmzTffXN68/vrry5tjWWGFFUbpnnzyyeXN97znPeXNF73oReXNJHnwwQdH6Y5hGIby5v7771/ePOSQQ8qbSXLssceWN2+//fby5qxZs8qbSXL88ceXN/faa6/yZpLssMMO5c1zzjmnvLnMMsuUN8ey2mqrlTdXXXXV8maSvPrVry5vbrbZZuXNJPnGN74xSrfaD37wg1G6Y/wa+MAHPlDeTJJ99923vDl9+vTy5lh++ctflje333778uY///M/lzeTZI899ihvjvHvP5ZXvepV5c2Xvexl5c1knN9bjPEaMKn//cXBBx/cvc8VBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKBr0qJ8sFVXXTX77LNPafPyyy8v7S3wyU9+srz5mte8prx51VVXlTfH8otf/GKU7jrrrFPevPLKK8ubRx99dHkzSfk5NaZPf/rT5c099tijvLnCCiuUN5Pk9ttvL2++8Y1vLG/ut99+5c0k2WWXXcqbN9xwQ3kzSV7xileUN7/3ve+VN6+77rry5lh+85vflDf//u//vryZJHvttVd5c6zn6y233HKUbrWpU6eO0j3yyCPLmwcccEB5M0kuuuii8ub8+fPLm2MZhqG8+dGPfrS8OWvWrPJmMs7n69tuu628OZaVVlqpvLnuuuuWN5Nxfs9y8cUXlzeT5CUveUlpb/nll+/e54oDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALomLcoHmzdvXo499tjS5l/91V+V9hY466yzyptf+tKXypvTp08vbybJrFmzypu33XZbeTNJll122fLmC1/4wvLmSiutVN5MkhVXXHGU7hi22GKL8uaee+5Z3nz3u99d3kySE088sby53377lTfHMnny5PLmaqutVt5MktmzZ5c3Z8yYUd5cZZVVyptjOfDAA8ubp512WnkzSf70T/+0vPmCF7ygvJkkn/3sZ8ubb3nLW8qbN954Y3kzSaZOnVre/MxnPlPeTJL111+/vLn55puXN8fyZ3/2Z+XN+fPnlzf333//8maSfOhDHypvfupTnypvjmXvvfcub955553lzSQ5/PDDy5tjvAZKkk022aS0t9RSS3Xvc8UBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6GrDMCy6B2tt0T0YAAAA8KQNw9Ae73ZXHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABA16SF+cmtteuTzE/yuyQPD8OwUcVBAQAAAEuGhRoOJmw1DMMdBR0AAABgCeOvKgAAAABdCzscDEn+n9baZa21GY/3Aa21Ga212a212Qv5WAAAAMAi1oZhePo/ubU/Hobh5tba85JclORvhmGY+b98/NN/MAAAAGA0wzC0x7t9oa44GIbh5okfb0tyTpJXLUwPAAAAWLI87eGgtfac1toKC95Osk2SK6oODAAAAFj8Fua7Kqye5JzW2oLOl4dh+F7JUQEAAABLhIX6GgdP+cF8jQMAAABYIo3yNQ4AAACAP2yGAwAAAKDLcAAAAAB0GQ4AAACAroX5rgpP2YYbbpgf/OAHpc0ddtihtLfAzJkzy5vbb799eXP69OnlzST5yEc+Mkp3DPPnzy9vHnPMMeXNtdZaq7yZJC996UvLm1tttVV5M0mmTp1a3nz5y19e3jzooIPKm0ly9dVXlzf33nvv8uaznjXOpjxlypTy5sYbb1zeTJIPfOAD5c1PfvKT5c0vfOEL5c0kWXfddcub3/te/Tde+tnPflbeTJJDDjmkvDnWefUnf/In5c1rr722vLnbbruVN5NknXXWKW9+7WtfK28myfe///3y5mtf+9ry5ty5c8ubSfJ3f/d35c2ddtqpvPnxj3+8vJmM89915513Lm8m4zwHHnbYYeXNY489tryZJJtuuml583nPe155M0nOO++8UbqPxxUHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHRNWpQPdsMNN+TAAw8sbU6aNM6/wn/+53+WN4877rjy5rve9a7y5lj+/d//fZTue9/73vLm2WefXd6cNm1aeTNJjjnmmFG6YzjiiCPKmzvuuGN58+ijjy5vJslPf/rT8uYHP/jB8uZYfv7zn5c3x/hvmiRnnHFGeXPbbbctb95///3lzbGM8RzwyCOPlDeT5Jvf/GZ5s7VW3kyS9dZbr7x57bXXljfH+n/14IMPljeXW2658maSnHnmmeXNF77wheXNuXPnljeT5LnPfW55c4z/pp///OfLm0ly0kknlTfHOq/GMMbnqpkzZ5Y3k+RTn/pUefPTn/50eTNJPvzhD5f2/umf/ql7nysOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQNekRflgDzzwQK699trS5qabblraW+CQQw4pb/7Lv/xLefNzn/tceTNJfvzjH5c377777vJmklxxxRXlzc985jPlzSuvvLK8mSRbbbXVKN0xrLPOOuXNtdZaq7y57bbbljeT5JhjjilvTp8+vby58sorlzeT5Etf+lJ58/DDDy9vJslBBx1U3jz++OPLm9ddd115cyzVz/9J8n/+z/8pbybJW97ylvLmdtttV95MxvkccOGFF5Y3v/3tb5c3k+Too48ub375y18ubybJmWeeWd7cbbfdyptj/b968MEHy5tjvAZ6/etfX95MkpNOOqm8OcbzyljOP//88uYRRxxR3kySG264obz50pe+tLyZJHPnzi3tPfDAA937XHEAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAuiYtygebOnVqPv7xj5c2r7vuutLeAnPnzi1vrrDCCuXN7373u+XNsZxwwgmjdLfeeuvy5qGHHlrenDZtWnkzSXbZZZdRumPYZpttyptHH310efP6668vbybJ5z//+fLmzJkzy5tj+dWvflXeHOs54MQTTyxvnnLKKeXN2bNnlzefSR566KFRuptvvnl5c+211y5vJs+czwHf//73R+m+8pWvLG/ut99+5c0k+cY3vlHefN3rXlfeHMu1115b3mytlTc33XTT8maS3HLLLeXN3/3ud+XNsbzxjW8sb86fP7+8mSRbbLFFefOcc84pbybJnDlzSnv3339/9z5XHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQNWlRPti9996bn/3sZ6XNWbNmlfYWeMMb3lDePOuss8qb06dPL2+O5Te/+c0o3be+9a3lzV133bW8edxxx5U3k3F+rZ5zzjnlzSTZcccdy5sf+MAHyptf+cpXyptJcuGFF5Y3jznmmPLmWO68887y5jbbbFPeTJJrrrmmvDllypTy5kknnVTeTJILLrigvHnYYYeVN+fMmVPeTJLPfe5z5c0vf/nL5c0k+epXvzpKt9rMmTNH6W688cblzcsuu6y8mSSvfe1ry5vLL798efNrX/taeTNJdtttt/LmoYceWt6cPHlyeTNJDj/88PLmXnvtVd4cywtf+MLy5ljPAf/1X/9V3vzEJz5R3kyS008/vbR34okndu9zxQEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdD3hcNBaO7W1dltr7YrH3LZya+2i1tovJ3587riHCQAAACwOT+aKg9OTbPd7tx2a5OJhGNZLcvHE+wAAAMAfmCccDoZhmJnkrt+7eeckZ0y8fUaSNxYfFwAAALAEeLpf42D1YRjmTbx9a5LVex/YWpvRWpvdWpv9P//zP0/z4QAAAIDFYaG/OOIwDEOS4X+5/5RhGDYahmGj5ZdffmEfDgAAAFiEnu5w8OvW2ppJMvHjbXWHBAAAACwpnu5wcF6SvSbe3ivJuTWHAwAAACxJnsy3Y/xKkh8leVFr7abW2j5JjkvyutbaL5O8duJ9AAAA4A/MpCf6gGEYduvc9ZriYwEAAACWMAv9xREBAACAP1yGAwAAAKDLcAAAAAB0GQ4AAACArjYMw6J7sNYW3YMBAAAAT9owDO3xbnfFAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdkxbpg02alFVWWaW0OW/evNLeAvvvv395c/311y9vzp8/v7yZJIcffnh58x/+4R/Km0ly4YUXljfPOeec8ubMmTPLm0nyoQ99qLw5a9as8maSHHzwweXNu+66q7z5N3/zN+XNJJk2bVp5c8MNNyxvXnHFFeXNJNl1113Lm+eff355M0ne8pa3lDfPOOOM8uakSeM8jT/88MPlzdNOO628+ZOf/KS8mSRvfvOby5sbbLBBeTNJvvjFL5Y33//+95c377777vJmMs7rlVNOOaW8mST77rtvefP5z39+efOggw4qbybJ3/7t35Y3h2Eob7797W8vbyZJa628ecABB5Q3k+Tcc88tb26//fblzdVXX728mSRXXXVVefNjH/tYeTNJ7rjjjtLeYYcd1r3PFQcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACga9KifLCVVlopb3jDG0qbv/jFL0p7C+yxxx7lzTvvvLO8Wf3fc0yTJ08epfuJT3yivPnggw+WN88555zyZpK84x3vKG/OmjWrvJkkSy+9dHlzzpw55c2f//zn5c0k2XLLLcubv/nNb8qbYzn33HPLm+985zvLm0lyyimnlDef97znlTcffvjh8maS3HXXXeXNk08+ubx59913lzeTZP78+eXNnXbaqbyZJKutttoo3Wq77rrrKN2Pf/zj5c0vfelL5c0kWWGFFcqbL3nJS8qbY9loo43Km2O8tv7JT35S3kySww8/vLw5xmugJHnWs+r/bPmDH/xgefPqq68ubybJqquuWt685ZZbyptJ8sMf/rC09789/7niAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHS1YRgW2YOtv/76w+mnn17avO2220p7C9x+++3lzU033bS8OXv27PJmkuyxxx7lzb/8y78sbybJ5ptvXt5cYYUVyptf//rXy5tJcskll5Q377jjjvJmkozx+earX/1qefPkk08ubybJ+9///vLmDTfcUN5873vfW95MkmOOOaa8OdZzwK233lreXH311cubc+bMKW8mycUXX1ze3G677cqbDzzwQHkzSd7xjneUN8f6f/XRj350lG61MV5XJMkZZ5xR3jzppJPKm0ny2c9+trx57bXXljcffPDB8maSzJs3r7z5nOc8p7y51157lTeTZMMNNyxvfv7zny9vJsmNN95Y3txxxx3Lm9/5znfKm8k45+qHP/zh8maS/PrXvy7tvfrVr87s2bPb493nigMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAuiYtygd76KGHcsstt5Q2p0yZUtpbYJdddilvHn/88eXNnXbaqbw5lksvvXSU7sorr1zevOuuu8qbd999d3kzSTbeeOPy5gUXXFDeTJKXv/zl5c3p06eXN2fMmFHeTJL/+I//KG9eddVV5c2xTJ48eXEfwpN21llnlTfXWWed8ua0adPKm2P5xS9+Ud687rrryptJcsIJJ5Q33/rWt5Y3k2TmzJnPiOb73ve+8maSfO1rXytvvuc97ylvJsnUqVPLm29+85vLm1/5ylfKm0my9957lzfHOK+WX3758maSbLvttuXN73znO+XNJLnxxhvLm7NmzSpv7rbbbuXNJDn//PPLm6uttlp5M0k222yz0t7/9lztigMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABA1xMOB621U1trt7XWrnjMbR9prd3cWvvZxD87jHuYAAAAwOLwZK44OD3Jdo9z+6eGYdhg4p/v1h4WAAAAsCR4wuFgGIaZSe5aBMcCAAAALGEW5mscHNBa+/nEX2V4btkRAQAAAEuMpzscfC7JC5JskGRekr/rfWBrbUZrbXZrbfY999zzNB8OAAAAWBye1nAwDMOvh2H43TAMjyT5fJJX/S8fe8owDBsNw7DRlClTnu5xAgAAAIvB0xoOWmtrPubdNyW5ovexAAAAwDPXpCf6gNbaV5JsmWTV1tpNSY5KsmVrbYMkQ5Lrk+w74jECAAAAi8kTDgfDMOz2ODd/YYRjAQAAAJYwC/NdFQAAAIA/cIYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQ9YTfVaHSTTfdlIMPPri0+Z73vKe0t8Db3/728ubb3va28uaXv/zl8uZYbrrpplG65557bnnz5ptvLm/usssu5c0kOeqoo8qbF1xwQXkzSbbffvvy5rRp08qb8+bNK28mye23317e3GGHHcqbY/3/v+WWW8qb9913X3kzSTbbbLPy5sc+9rHy5plnnlneHMvZZ59d3jz//PPLm0ly7733ljfvuuuu8maSrLzyyqN0qy2zzDKjdA8//PDy5p577lneTJJ3vvOd5c1n0uvAE044obw5d+7c8uZWW21V3kySV7/61eXNMV5XjWX99dcvb471HDjGa+sPfvCD5c2k/rXFs57Vv67AFQcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdLVhGBbdg7W26B4MAAAAeNKGYWiPd7srDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAEDXpEX5YK985Stz6aWXljbf+ta3lvYW+N3vflfe/OY3v1nePPXUU8ubSfKOd7yjvDlv3rzyZpIceeSR5c0f/OAH5c13vetd5c0kWXHFFcube++9d3lzLMMwlDf33Xff8uZYdtttt/LmVlttVd4cyxprrDFKd/fddy9vzpo1q7z5k5/8pLyZjHNejfG8stJKK5U3k2SLLbYob+60007lzST5x3/8x/LmfvvtV9585JFHypvJOJ+vN9hgg/JmkrzoRS8qb6633nrlzec///nlzSTZZ599yptnnXVWeXOs10Brr712eXOsX6uvec1rypv/+q//Wt7ceuuty5tJcvbZZ5c3L7vssvJmknz7298u7c2dO7d7nysOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZT9oAq0AAAtySURBVDgAAAAAugwHAAAAQNekRflgv/rVr7LffvuVNm+66abS3gJz584tb/7FX/xFefPee+8tb46ltTZK9/jjjy9vnnnmmeXNrbfeuryZJBdddNEo3TH86Ec/Km8eddRR5c2xzqs5c+aUNz/ykY+UN8dyyCGHlDePO+648maSfOpTnypvnnvuueXN8847r7yZJDNmzChvHnzwweXNW265pbyZjHNeff3rXy9vJsmpp55a3qx+rZYk++yzT3kzGefz9dJLL13eTJJll122vHnPPfeUN8ey7rrrljcnT55c3txkk03Km8k4r4FWXHHF8uZYpkyZUt7ceeedy5tJstpqq5U3n/Oc55Q3k+R1r3tdae/WW2/t3ueKAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6Ji3KB3v2s5+dNddcs7T52te+trS3wGmnnVbePOSQQ8qb5513XnkzSS644ILy5llnnVXeTJI3velN5c3dd9+9vDllypTyZpK8+MUvHqU7hjlz5pQ3L7/88vLm9ttvX95Mkn333be8eeSRR5Y3x3L00UeXN9dff/3yZpKcffbZ5c3llluuvDkMQ3lzLB/96EfLm0cccUR5M0mOOuqo8uY555xT3kySpZdeepRutTGeV5Pk4osvLm++853vLG8m47xmnTFjRnlzLIcddlh584orrihvvvSlLy1vJskvf/nL8uY111xT3hzLZZddVt48/fTTy5tJst9++5U377nnnvJmktx///2lvXvvvbd7nysOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF1POBy01tZqrf1ba+2q1tqVrbUDJ25fubV2UWvtlxM/Pnf8wwUAAAAWpSdzxcHDST4wDMO0JJsk2b+1Ni3JoUkuHoZhvSQXT7wPAAAA/AF5wuFgGIZ5wzD8dOLt+UnmJPnjJDsnOWPiw85I8saxDhIAAABYPJ7S1zhora2TZMMklyRZfRiGeRN33Zpk9c7PmdFam91am33vvfcuxKECAAAAi9qTHg5aa8sn+WaS9w7DcM9j7xuGYUgyPN7PG4bhlGEYNhqGYaPllltuoQ4WAAAAWLSe1HDQWnt2Hh0NzhyG4eyJm3/dWltz4v41k9w2ziECAAAAi8uT+a4KLckXkswZhuHvH3PXeUn2mnh7ryTn1h8eAAAAsDhNehIfs1mSPZJc3lr72cRtH0pyXJKzWmv7JPlVkreOc4gAAADA4vKEw8EwDLOStM7dr6k9HAAAAGBJ8pS+qwIAAADwfxfDAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6Hoy346xzG9/+9tccsklpc2Xvexlpb0FjjzyyPLm9ddfX978/ve/X94cy8477zxK95hjjilvzp07t7w5ffr08maSbLLJJuXNH//4x+XNJHnb295W3jz//PPLm2ussUZ5M0m++93vljeXXXbZ8uZYPvGJT5Q3L7300vJmknznO98pb5544onlzW222aa8OZb58+eXN2fMmFHeTJI99tijvLnxxhuXN5Pk7rvvLm+utNJK5c2LLrqovJkkV1xxRXnzTW96U3kzSdZZZ53y5v3331/eHMuFF15Y3txiiy3KmxdccEF5M0nWXnvt8uYYr6vGcvnll5c3d9hhh/Jmkuy6667lzZVXXrm8mdR/Drzmmmu697niAAAAAOgyHAAAAABdhgMAAACgy3AAAAAAdBkOAAAAgC7DAQAAANBlOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAICuSYvywVZfffW8733vK22ef/75pb0FTjrppPLmX//1X5c3t9lmm/JmksyaNau8ecQRR5Q3k+TP//zPy5sPPfTQM6KZPHpePVOMcb7+93//d3lzvfXWK28myU9/+tPy5gte8ILy5ljuu+++8uaMGTPKm0myzDLLlDfXX3/98uaqq65a3hzLj370o/Jm9WuKBfbff//y5ve+973yZpKcfPLJo3Sr3XrrraN0d9555/LmYYcdVt5MktNOO628ucYaa5Q3xzJlypTy5g9/+MPy5p133lneTJJ3v/vd5c2xngNPOeWU8ua2225b3pw6dWp5MxnnteXMmTPLm0my1lprlfae9az+dQWuOAAAAAC6DAcAAABAl+EAAAAA6DIcAAAAAF2GAwAAAKDLcAAAAAB0GQ4AAACALsMBAAAA0GU4AAAAALoMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAIAuwwEAAADQZTgAAAAAugwHAAAAQJfhAAAAAOgyHAAAAABdbRiGRfdgrd2e5FdP8sNXTXLHiIcD/7dxTkE95xXUc15BLecUT9bzh2FY7fHuWKTDwVPRWps9DMNGi/s44A+FcwrqOa+gnvMKajmnqOCvKgAAAABdhgMAAACga0keDk5Z3AcAf2CcU1DPeQX1nFdQyznFQltiv8YBAAAAsPgtyVccAAAAAIuZ4QAAAADoWuKGg9badq21/26tXd1aO3RxHw88E7XWTm2t3dZau+Ixt63cWruotfbLiR+fuziPEZ5JWmtrtdb+rbV2VWvtytbagRO3O6/gaWqtTW6t/aS19l8T59XRE7f/SWvtkonXgl9rrS29uI8Vnmlaa0u11v6ztfbtifedVyyUJWo4aK0tleTkJNsnmZZkt9batMV7VPCMdHqS7X7vtkOTXDwMw3pJLp54H3hyHk7ygWEYpiXZJMn+E89Pzit4+h5IsvUwDC9PskGS7VprmyT5eJJPDcPwp0l+k2SfxXiM8Ex1YJI5j3nfecVCWaKGgySvSnL1MAzXDsPwYJKvJtl5MR8TPOMMwzAzyV2/d/POSc6YePuMJG9cpAcFz2DDMMwbhuGnE2/Pz6Mvxv44zit42oZH/c/Eu8+e+GdIsnWSb0zc7ryCp6i1NjXJjkn+aeL9FucVC2lJGw7+OMmNj3n/ponbgIW3+jAM8ybevjXJ6ovzYOCZqrW2TpINk1wS5xUslInLqX+W5LYkFyW5JsndwzA8PPEhXgvCU/fpJAcneWTi/VXivGIhLWnDAbAIDI9+H1bfixWeotba8km+meS9wzDc89j7nFfw1A3D8LthGDZIMjWPXnn64sV8SPCM1lp7fZLbhmG4bHEfC39YJi3uA/g9NydZ6zHvT524DVh4v26trTkMw7zW2pp59E93gCeptfbsPDoanDkMw9kTNzuvoMAwDHe31v4tyfQkK7XWJk386ajXgvDUbJZkp9baDkkmJ5mS5DNxXrGQlrQrDi5Nst7EV/1cOsmuSc5bzMcEfyjOS7LXxNt7JTl3MR4LPKNM/P3QLySZMwzD3z/mLucVPE2ttdVaaytNvL1sktfl0a8f8m9J3jLxYc4reAqGYThsGIapwzCsk0d/L/WvwzDsHucVC6k9emXlkmNiHft0kqWSnDoMw7GL+ZDgGae19pUkWyZZNcmvkxyV5FtJzkqydpJfJXnrMAy//wUUgcfRWts8yb8nuTz/398Z/VAe/ToHzit4GlprL8ujX6RtqTz6h1lnDcPwt621dfPoF8heOcl/JvnrYRgeWHxHCs9MrbUtk3xwGIbXO69YWEvccAAAAAAsOZa0v6oAAAAALEEMBwAAAECX4QAAAADoMhwAAAAAXYYDAAAAoMtwAAAAAHQZDgAAAICu/xdAPj+brEg3owAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you want to see the output of an image passing through first convolution layer?"
      ],
      "metadata": {
        "id": "AhQNvB7zCbzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
        "sampleimage = [x_image_train[0]]\n",
        "plt.imshow(np.reshape(sampleimage,[28,28]), cmap=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "N55LAxoHCcMT",
        "outputId": "41aec14c-7135-4c46-e276-1bdc595b621f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fee52178350>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fBmcQTL3Hu7R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}